---
title: ReadPaper
date: 2025-07-25 12:50:48
---

<style>
  html, body, .markdown-body {
    font-family: Georgia, sans, serif;
  }
</style>


<div class="markdown-body">

# Recordings of Reading Paper

- `2025/07`:

    - `2025/07/26`:

        - Learning technical blog written by Lilian Weng, [Why we think](https://lilianweng.github.io/posts/2025-05-01-thinking/), forthe first section. (It is too long...)

    - `2025/07/27` - `2025/07/28`:
        - Rereading and writing the summary notes for the classical transformer: **Attention is all you need**.

        - Thought: Attention pooling is the core component for understanding the power for transformer model, where context meaning is learning via the process of self-attention.
</div>