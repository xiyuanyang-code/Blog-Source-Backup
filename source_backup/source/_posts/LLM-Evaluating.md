---
title: LLM Evaluating
date: 2025-05-30 15:31:49
index_img: /img/cover/llmeva.png
excerpt: Initial try for LLM evaluating on gsm8k dataset.
categories:
  - Artificial Intelligence
tags:
  - Tutorial
  - Finished
  - LLM
  - Python
---

<style>
  html, body, .markdown-body {
    font-family: Georgia, sans, serif;
  }
</style>

# LLM Evaluating (from scratch)

## Introduction

æœ¬æ–‡å°†ä¼š**æ¼”ç¤ºä»é›¶å¼€å§‹**å¦‚ä½•éƒ¨ç½²æ„å»ºè‡ªå·±çš„æœ¬åœ°ï¼ˆæˆ–è€…åœ¨çº¿APIè°ƒç”¨ï¼‰LLMå¹¶ä½¿ç”¨Hugging Faceä¸Šçš„æ•°æ®é›†è¿›è¡Œè·‘åˆ†è¯„ä¼°ï¼Œä¹Ÿä½œä¸ºç¬”è€…åˆå…¥LLMé¢†åŸŸçš„å­¦ä¹ ç¬”è®°ä¹‹ä¸€ã€‚

ç›¸å…³ä»£ç å¼€æºåœ¨ [æˆ‘çš„ä»“åº“](https://github.com/xiyuanyang-code/LLM_Learn/tree/master/evaluation) ä¸­ã€‚

## Table of contents

æˆ‘ä»¬çš„ä»£ç ç»“æ„å¦‚ä¸‹ï¼š

```plaintext
.
â”œâ”€â”€ data
â”œâ”€â”€ img
â”œâ”€â”€ results
â””â”€â”€ src
    â”œâ”€â”€ app.py
    â”œâ”€â”€ download.py
    â”œâ”€â”€ evaluate.py
    â”œâ”€â”€ main.py
    â””â”€â”€ utils.py
```

å¯ä»¥çœ‹åˆ°

## Downloading Datasets

æˆ‘ä»¬ä½¿ç”¨**Hugging Face**ä¸‹è½½æ•°æ®é›†ï¼Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ä½¿ç”¨`openai/gsm8k`çš„ç»å…¸æ•°æ®é›†ï¼Œæ¥è¯„æµ‹æœ¬åœ°å¤§æ¨¡å‹`Llama-3 8B`çš„è¯„åˆ†è¡¨ç°ã€‚

```python
def download_dataset(dataset="openai/gsm8k"):
    """download the dataset online and revert to json file"""
    ds = load_dataset(dataset, "main")

    for split in ds:
        data = list(ds[split])

        with open(f"{split}.json", "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
```

ä¸Šé¢çš„å‡½æ•°ä½¿ç”¨äº†`dataset`åº“ï¼Œæ•…éœ€è¦åŠ ä¸Š`from datasets import load_dataset`æ¥å¯¼å…¥ç›¸å…³çš„åº“å‡½æ•°ã€‚æ¥ç€è®²è¿™ä¸ªå†…å®¹å­˜å‚¨ä¸º`json`æ ¼å¼å­˜å‚¨åœ¨æœ¬åœ°ä¹‹åè¯»å–ã€‚

å½“ç„¶ï¼Œå¯¹äºè¾ƒå¤§çš„æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä¸‹è½½åˆ°æœ¬åœ°ä¸ºjsonæ ¼å¼ï¼Œå‚è§ [Hugging Faceæ•°æ®é›†ä¸‹è½½](https://xiyuanyang-code.github.io/posts/My-Memo/#Hugging-Face-%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8B%E8%BD%BD)ã€‚

## Preparing Models

### Downloading Models and activating

æˆ‘ä»¬åœ¨è¿™é‡Œä½¿ç”¨**Llama3-8B**çš„ä¸­å°å‹æ¨¡å‹ï¼Œä½¿ç”¨æœ¬åœ°éƒ¨ç½²ï¼ˆå»ºè®®è‡³å°‘æœ‰ä¸€å—30ç³»ä»¥ä¸Šçš„æ˜¾å¡ï¼‰ï¼Œéƒ¨ç½²çš„è¯¦ç»†è¿‡ç¨‹å‚è§ï¼š

[Datawhale å¤§æ¨¡å‹æœ¬åœ°éƒ¨ç½²](https://github.com/datawhalechina/self-llm/blob/master/models/LLaMA3/01-LLaMA3-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md)

### Preparing API keys

å¯¹äºé—­æºå¤§æ¨¡å‹å’Œç¼ºä¹ç¡¬ä»¶æ”¯æŒçš„ç©å®¶æ¥è¯´ï¼Œä½¿ç”¨**APIå¯†é’¥**è¿›è¡ŒLLMè°ƒç”¨æ˜¯éå¸¸æ–¹ä¾¿ä¸”é«˜æ•ˆçš„é€‰æ‹©ã€‚

å¯¹äºå›½å†…ç©å®¶ï¼Œéå¸¸æ¨èä½¿ç”¨ [æ™ºå¢å¢](https://gpt.zhizengzeng.com/#/my/) ä½œä¸ºAPIæ¥å£çš„ç¬¬ä¸‰æ–¹å¹³å°ã€‚åœ¨è¿™ä¸ªç½‘ç«™ä¸Šç”³è¯·å¾—åˆ°API Keyä¹‹åï¼Œæˆ‘ä»¬å°±å¯ä»¥å‡†å¤‡æ¿€æ´»è‡ªå·±çš„LLMäº†ã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª`.env`æ–‡ä»¶ï¼Œç”¨äºåç»­è¯»å–`OPENAI_API_KEY`å’Œ`BASE_URL`ï¼Œå½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥å†™åœ¨ç¯å¢ƒå˜é‡ä¹‹ä¸­ï¼Œä¸€åŠ³æ°¸é€¸ã€‚

```python
def init_env_file(
    env_path=".env",
    default_content='OPENAI_API_KEY="1234567 (replace with your own)"\nBASE_URL="example.chat (replace with your own)"',
):
    if not os.path.exists(env_path):
        with open(env_path, "w", encoding="utf-8") as f:
            f.write(default_content)
        print("WARNING, remember to use your own api secret key")


def load_api_key():
    # init api file
    init_env_file()

    import dotenv
    dotenv.load_dotenv(override=True)
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    BASE_URL = os.getenv("BASE_URL")
    # print(OPENAI_API_KEY)
    # print(BASE_URL)
    return OPENAI_API_KEY, BASE_URL
```

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°`app.py`å³æ„å»ºLLMå›è¯çš„æ¥å£å‡½æ•°ã€‚

```python
import os
import requests
import json
from utils import load_api_key


def get_completions_online(prompt):
    from openai import OpenAI

    OPENAI_API_KEY, BASE_URL = load_api_key()
    # get prompts:
    added_prompts = "For the final answer you have given , please return: <answer>Your final answer</answer> at the end of your response, remember you are only allowed to return a float number! For example, 18 or 18.0 is allowed but $18 is not allowed"

    client = OpenAI(api_key=OPENAI_API_KEY, base_url=BASE_URL)
    resp = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt + added_prompts},
        ],
    )
    return resp.choices[0].message.content


def get_completions_offline(prompt):
    """get completions from the local host LLMs

    Args:
        prompt (str): Given prompts into the LLM
        max_length (int, optional): max length ofn single query. Defaults to 100.

    Returns:
        str: final response from the model
    """
    headers = {"Content-Type": "application/json"}
    max_length = 100

    # get prompts:
    added_prompts = "For the final answer you have given , please return: <answer>Your final answer</answer> at the end of your response, remember you are only allowed to return a float number! For example, 18 or 18.0 is allowed but $18 is not allowed"

    data = {"prompt": prompt + added_prompts, "max_length": max_length}
    response = requests.post(
        url="http://127.0.0.1:6005",
        headers=headers,
        data=json.dumps(data),
        proxies={"http": None, "https": None},
    )

    return response.json().get("response", "No response from model")


def get_completions(prompt, type: str = "offline"):
    if type == "online":
        return get_completions_online(prompt)
    elif type == "offline":
        return get_completions_offline(prompt)


if __name__ == "__main__":
    print(get_completions_online("Hello, introduce yourself"))

```

## Evaluating

å‡†å¤‡å¥½äº†LLMå’Œæ•°æ®é›†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¯¥ç»™LLMè¯„æµ‹äº†ï¼Œç›¸å…³è¯„æµ‹çš„ä»£ç å†™åœ¨äº†`evaluat.py`å‡½æ•°é‡Œé¢ã€‚

```python
import requests
import json
import re

from tqdm import tqdm
from app import get_completions


def test_evaluate(dataset):
    """a small test function for testing LLM's response

    Args:
        dataset (List): the loaded dataset from function load_local_dataset
    """
    get_completions("Hello, introduce yourself!")
    test_results = []
    single_evaluate(test_results, dataset[0])
    print(test_results)


def single_evaluate(results: list, item, status):
    """single evaluation for single query

    Args:
        results (list): result list
        item (dict): a single query from dataset
    """
    prompt: str = item["question"]
    expected_answer: str = item["answer"]

    # get LLM's response
    llm_response: str = get_completions(prompt, status)

    match = re.search(r"<answer>(.*?)</answer>(.*)", llm_response)

    # exception handling
    if match is None:
        final_answer = "LLM fails to response"
        results.append(
            {
                "prompt": prompt,
                "expected_answer": expected_answer,
                "model_response": llm_response,
                "final_answer": final_answer,
                # None means LLM fails to output in the right format
                "correct": None,
            }
        )
    else:
        final_answer = float(match.group(1).replace(",", ""))
        results.append(
            {
                "prompt": prompt,
                "expected_answer": expected_answer,
                "model_response": llm_response,
                "final_answer": final_answer,
                "correct": final_answer
                == float(expected_answer.split("####")[-1].strip().replace(",", "")),
            }
        )


# run tests and evaluate models
def evaluate(datasets, status):
    results = []
    for item in tqdm(datasets, desc="Evaluating"):
        single_evaluate(results=results, item=item, status=status)
    return results


def calculate_metrics(results):
    total = len(results)
    correct = sum(1 for result in results if result["correct"] is True)
    no_answer = sum(1 for result in results if result["correct"] is None)
    accuracy = correct / total if total > 0 else 0
    no_answer_rate = no_answer / total if total > 0 else 0
    return {
        "total": total,
        "correct": correct,
        "accuracy": accuracy,
        "no_answer_rate": no_answer_rate,
    }

```

## Main Function

```python
import argparse
import json
from datetime import datetime
from download import download_dataset, download_model, load_local_datasets
from utils import construct_folder, check_construct, split_print
from evaluate import evaluate, calculate_metrics


def main(status):
    print("Welcome! It is LLM 101")

    # section 1: construct and preparations
    folders = ["data", "src", "img", "results"]
    construct_folder(folders)
    assert check_construct(folders)

    # section 2: activate LLMs
    # ! let just skip that part
    # model_dir = download_model()
    # ! please run the model in the certain port, see https://github.com/datawhalechina/self-llm/blob/master/models/LLaMA3/01-LLaMA3-8B-Instruct%20FastApi%20%E9%83%A8%E7%BD%B2%E8%B0%83%E7%94%A8.md for reference
    # ! we assume that your model is available in localhost: 6005

    # section 3: download_data
    # download_dataset()
    datasets = load_local_datasets("data/test.json")

    # section 4: evaluate
    timestamp = datetime.now().strftime("%Y%m%d%H%M")
    results = evaluate(datasets, status=status)
    ans = calculate_metrics(results=results)

    split_print()
    print("Accuracy: ", ans["accuracy"])
    print("No answer rate: ", ans["no_answer_rate"])
    split_print()

    # section 5: save answers
    result_path = f"results/ans_{timestamp}.json"
    response_path = f"results/response_{timestamp}.json"

    with open(result_path, "w", encoding="utf-8") as f:
        json.dump(ans, f, ensure_ascii=False, indent=2)

    with open(response_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"Results saved to {result_path}")
    print(f"All the responses for the LLM saved to {response_path}")


if __name__ == "__main__":
    # add argparse
    parser = argparse.ArgumentParser(description="Add arguments for LLM evaluating")

    parser.add_argument(
        "--type", type=str, default="online", help="ways to load LLM, online or offline"
    )
    args = parser.parse_args()

    status = "online" if args.type == "online" else "offline"
    main(status)

```

# Another Example!

è¿™æ¬¡ï¼Œæˆ‘ä»¬éƒ¨ç½²**æ®‹è¡€ç‰ˆ**çš„DeepSeekæ¨¡å‹ï¼ˆ16Bçš„æ¨¡å‹ï¼Œå…¶å®è¿˜æ˜¯åå°çš„ï¼‰

åŒæ ·ï¼Œæˆ‘ä»¬åœ¨HuggingFaceä¸‹è½½å¥½DeepSeekæ¨¡å‹ä¹‹åï¼Œä½¿ç”¨å¦‚ä¸‹çš„`app.py`ä»£ç å¯åŠ¨ç›‘å¬ç«¯å£ï¼š

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import uvicorn
import json
import datetime
import torch
import os

os.environ["CUDA_VISIBLE_DEVICES"] = "3,4,5,6"
# Set device parameters
DEVICE = "cuda"  # Use CUDA
DEVICE_ID = "0"  # CUDA device ID, leave empty if not set
CUDA_DEVICE = (
    f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE
)  # Combine CUDA device info


# Function to clear GPU memory
def torch_gc():
    if torch.cuda.is_available():  # Check if CUDA is available
        with torch.cuda.device(CUDA_DEVICE):  # Specify CUDA device
            torch.cuda.empty_cache()  # Clear CUDA cache
            torch.cuda.ipc_collect()  # Collect CUDA memory fragments


# Create FastAPI app
app = FastAPI()


# Endpoint to handle POST requests
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # Declare global variables for model and tokenizer
    json_post_raw = await request.json()  # Get JSON data from POST request
    json_post = json.dumps(json_post_raw)  # Convert JSON data to string
    json_post_list = json.loads(json_post)  # Convert string to Python object
    prompt = json_post_list.get("prompt")  # Get prompt from request
    max_length = json_post_list.get("max_length")  # Get max_length from request

    # Build messages
    messages = [{"role": "user", "content": prompt}]
    # Build input tensor
    input_tensor = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, return_tensors="pt"
    )
    # Generate output from model
    outputs = model.generate(input_tensor.to(model.device), max_new_tokens=max_length)
    result = tokenizer.decode(
        outputs[0][input_tensor.shape[1] :], skip_special_tokens=True
    )

    now = datetime.datetime.now()  # Get current time
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # Format time as string
    # Build response JSON
    answer = {"response": result, "status": 200, "time": time}
    # Build log information
    log = (
        "["
        + time
        + "] "
        + '", prompt:"'
        + prompt
        + '", response:"'
        + repr(result)
        + '"'
    )
    print(log)  # Print log
    torch_gc()  # Perform GPU memory cleanup
    return answer  # Return response


# Main entry point
if __name__ == "__main__":
    model_name_or_path = (
        "/GPFS/rhome/xiyuanyang/local_LLM/deepseek-ai/deepseek-moe-16b-chat"
    )
    # Load pretrained tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(
        model_name_or_path, trust_remote_code=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    model.generation_config = GenerationConfig.from_pretrained(model_name_or_path)
    model.generation_config.pad_token_id = model.generation_config.eos_token_id
    model.eval()  # Set model to evaluation mode
    # Start FastAPI app
    # Using port 6006 allows port mapping from autodl to local, so the API can be used locally
    uvicorn.run(
        app, host="0.0.0.0", port=6000, workers=1
    )  # Start app on specified host and port

```

æ¥ç€ä½ å°±å¯ä»¥çœ‹åˆ°ï¼š

```plaintext
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:19<00:00,  2.73s/it]
INFO:     Started server process [525302]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:6000 (Press CTRL+C to quit)
```

ä½¿ç”¨FastAPIåœ¨ç‰¹å®šçš„ç›‘å¬ç«¯å£ç›‘å¬ï¼š

```python
import requests
import json


def get_completion(prompt, max_length):
    headers = {"Content-Type": "application/json"}
    data = {"prompt": prompt, "max_length": max_length}
    response = requests.post(
        url="http://127.0.0.1:6000", headers=headers, data=json.dumps(data)
    )
    return response.json()["response"]


if __name__ == "__main__":
    prompt = input()
    content: str = get_completion(prompt, 10000000)
    contents = content.strip().split("\n")
    for con in contents:
        print(con)

```

è¿™æ ·å°±å¯ä»¥å®ç°æœ€åŸºæœ¬çš„LLMå’Œç”¨æˆ·äº¤äº’çš„é€»è¾‘ã€‚

```plaintext
Please input your prompts.ğŸ˜˜
I am a student learning computer science and artificial intelligence. I want to learn the basic knowledge of front-back end developing in the summer vacation and become an excellent developer in the future, can you arrange me a plan for 40 dayas to improve my developing skills? I want to improve myself in aspects of how to conduct product research and task-planning, basic knowlwdge of front-back end developing and software engineering, generate a whole report for me. Thank you!
Sure, I can help you create a plan to improve your front-end and back-end developing skills and gain knowledge in product research and task planning. Here's a 4-step plan:

1. Learn the basics of front-end and back-end development:
* Front-end development: HTML, CSS, JavaScript, and frameworks like React, Angular, or Vue.js.
* Back-end development: Python, Java, or Ruby on Rails, and databases like MySQL, MongoDB, or PostgreSQL.
* Learn how to use version control systems like Git and GitHub.
2. Conduct product research and task planning:
* Learn how to conduct market research and user research to understand your target audience and their needs.
* Learn how to create user personas, user stories, and user journeys to understand user behavior and create a user-centered design.
* Learn how to create a project roadmap, including milestones, deadlines, and tasks.
3. Learn software engineering principles:
* Learn how to write clean, maintainable, and scalable code.
* Learn how to use design patterns and best practices for software development.
* Learn how to use agile methodologies like Scrum or Kanban for project management.
4. Generate a whole report:
* Create a report that summarizes your learning and skills gained during the summer vacation.
* Include a section on front-end and back-end development, including the technologies and frameworks you learned.
* Include a section on product research and task planning, including the methods and tools you used.
* Include a section on software engineering principles, including the design patterns and best practices you learned.
* Include a conclusion that summarizes your learning and future goals.

I hope this plan helps you improve your front-end and back-end developing skills and gain knowledge in product research and task planning. Good luck with your summer vacation!
```

